#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Templates:
#
# Nouns:
# -----
#
# aquila:
#
# {{la-noun|aquila|aquilae|f|first}}
# {{la-decl-1st|aquil}}
#
# abāctor:
#
# {{la-noun|abāctor|abāctōris|m|third}}
# {{la-decl-3rd|abactor}}
#
# abaculus:
#
# {{la-noun|abaculus|abaculī|m|second}}
# {{la-decl-2nd|abacul}}
#
# abdōmen:
#
# {{la-noun|abdōmen|abdōminis|n|third}}
# {{la-decl-3rd-N|abdōmen}}
#
# abomāsum:
#
# {{la-noun|abomāsum|abomāsī|n|second}}
# {{la-decl-2nd-N|abomās}}
#
# accipiter:
#
# {{la-noun|accipiter|accipitris|m|third}}
# {{la-decl-3rd|accipiter}}
#
# ȳ:
#
# {{la-noun|ȳ|indecl=yes}}
#
# dē:
#
# {{head|la|noun|head=dē|{{l|en|indeclinable}}}}
#
# absinthium:
#
# {{la-noun|absinthium|absinthiī|gen2=absinthī|n|second}}
# {{la-decl-2nd-N-ium|absinth}}
#
# lēns:
#
# {{la-noun|lēns|lentis|f|third}}
# {{la-decl-3rd|lēns}}
#
# diēs:
#
# {{la-noun|diēs|diēī|m|g2=f|fifth}}
# {{la-decl-5th-i|d}}
#
# ōs:
#
# {{la-noun|ōs|ōris|n|third}}
# {{la-decl-3rd-N-I|ōs|ōr}}
#
# os:
#
# {{la-noun|os|ossis|n|third}}
# {{la-decl-3rd-N-I|os|oss|gen_pl=ossium}}
#
# cōnsēnsus:
#
# {{la-noun|cōnsēnsus|cōnsēnsūs|m|fourth}}
# {{la-decl-4th|cōnsēns}}
#
# comma:
#
# {{la-noun|comma|commatis|n|third}}
# {{la-decl-3rd-N|comma}}
#
# colon:
#
# {{la-noun|colon|colī|n|second}}
# {{la-decl-2nd-N-Greek|col}}
#
# gas:
#
# {{la-noun|gas|gasis|n|third}}
# {{la-decl-3rd|gas|gas}}
#
# phoenīx:
#
# {{la-noun|phoenīx|phoenīcis|f|third}}
# {{la-decl-3rd-I|phoenīx}}
# {{la-noun|phoenīx|phoenīcis|m|third}}
# {{la-decl-3rd|phoenīx}}
#
# Proper nouns:
# -------------
#
# Belgium:
#
# {{la-proper noun|Belgium|Belgiī|n|second}}
# {{la-decl-2nd-N|num=sg|Belgi}}
#
# Isrāēl:
#
# {{la-proper noun|Isrāēl||m|indecl=1}}<br/>
# {{la-proper noun|Isrāēl|Isrāēlis|m|third}}
# {{la-decl-3rd|num=sg|Isrāēl|Isrāēl}}
#
# Abaddōn:
#
# {{head|la|proper noun|head=Abaddōn|g=m|indeclinable}}
#
# Andorra:
#
# {{la-proper noun|Andorra|Andorrae|f|first}}
# {{la-decl-1st|loc=1|num=sg|Andorr}}
#
# Niger:
#
# {{la-proper noun|Niger|Nigrī|m|second}}
# {{la-decl-2nd-er|Niger|Nigr}}
# {{la-proper noun|Niger|Nigris|m|third}}
# {{la-decl-3rd|num=sg|Niger|acc_sg=Nigrim}}
#
# Achilles:
#
# {{la-proper noun|Achilles|Achillis|m|third}}
# {{la-decl-3rd|num=sg|Achilles|Achill}}
#
# Pronouns:
# ---------
#
# is:
#
# {{head|la|pronoun}}
# {{la-decl-irreg|is}}
#
# Verbs:
# ------
#
# līberō:
#
# {{la-verb|līberō|līberāre|līberāvī|līberātum|conj=1}}
# {{la-conj-1st|līber|līberāv|līberāt|sync_perf=poet|p3inf=1}}
#
# ferō:
#
# {{la-verb|ferō|ferre|tulī|perf2=tetulī|lātum|conj=3|pattern=irreg}}
# {{la-conj-irreg|fero}}
#
# eō:
#
# {{la-verb|eō|īre|iī|itum|conj=irreg}}
# {{la-conj-irreg|eo|p3inf=1}}
#
# for:
#
# {{la-verb|for|fārī|fātus sum|conj=1|pattern=depon}}
# {{la-conj-1st|type=depon|f|fāt|p3inf=1}}
#
# abdīcō:
#
# {{la-verb|abdīcō|abdīcere|abdīxī|abdictum|conj=irreg}}
# {{la-conj-irreg|dico|ab}}
#
# abdō:
#
# {{la-verb|abdō|abdere|abdidī|abditum|conj=3}}
# {{la-conj-3rd|abd|abdid|abdit}}
#
# alō:
#
# {{la-verb|alō|alere|aluī|altum|sup2=alitum|conj=3}}
# {{la-conj-3rd|al|alu|alt}}
# {{la-conj-3rd|al|alu|alit}}
#
# lingō:
#
# {{la-verb|lingō|lingere|linxī|linctum|conj=3}}
# {{la-conj-3rd|ling|linx|linct}}
#
# vērō:
#
# {{la-verb|vērō|vērāre|conj=1|pattern=nopass-noperf}}
# {{la-conj-1st|type=nopass-noperf|vēr}}
#
# lābor:
#
# {{la-verb|lābor|lābī|lāpsus sum|conj=3|pattern=depon}}
# {{la-conj-3rd|type=depon|lāb|lāps}}
#
# videō:
#
# {{la-verb|videō|vidēre|vīdī|vīsum|conj=2}}
# {{la-conj-2nd|vid|vīd|vīs|p3inf=1}} 
#
# Prefixes:
# ---------
#
# ā-:
#
# {{head|la|prefix|head=ā-}}
#
# Prepositions:
# -------------
#
# in:
#
# {{la-prep|accusative|ablative}}
#
# Adjectives:
# -----------
#
# phoenīx:
#
# {{la-adj-3rd-1E|phoenīx|phoenīcis}}
# {{la-decl-3rd-1E|type=par|phoenīx|phoenīc}}
#
# incōmptus:
#
# {{la-adj-1&2|incōmptus|incōmpta|incōmptum}}
# {{la-decl-1&2|incōmpt}}
#
# Adverbs:
# --------
#
# lentē:
#
# {{la-adv|lentē}}
#
# ferē:
#
# {{la-adv|ferē|-}}
#
# Interjections:
# --------------
#
# ēn:
#
# {{la-interj|ēn}}
#
# tax:
#
# {{head|la|interjection}}
#
# Letters:
# --------
#
# o:
#
# {{head|la|letter}}
#
# Abbreviations:
# --------------
#
# JUL:
#
# {{head|la|abbreviation}}
#
# Adjective forms:
# ----------------
#
# {{la-adj-form|abdominālēs}}
# # {{inflection of|abdominālis||nom|m|p|lang=la}}
# # {{inflection of|abdominālis||nom|f|p|lang=la}}
# # {{inflection of|abdominālis||acc|m|p|lang=la}}
# # {{inflection of|abdominālis||acc|f|p|lang=la}}
# # {{inflection of|abdominālis||voc|m|p|lang=la}}
# # {{inflection of|abdominālis||voc|f|p|lang=la}}
#
# Verb forms:
# -----------
#
# {{la-verb-form|abdūce}}
#
# # {{inflection of|abdūcō||2|s|pres|actv|impr|lang=la}}
#
# Noun forms:
# -----------
#
# {{la-noun-form|diē}}
#
# # {{inflection of|diēs||abl|s|lang=la|nodot=1}} (&quot;[[day]]&quot;).
#
# Participle forms:
# -----------------
#
# abrāse:
#
# {{la-part-form|abrāse}}
#
# # {{inflection of|abrāsus||voc|m|s|lang=la}}
#
# Pronoun forms:
# --------------
#
# mī:
#
# {{la-pronoun-form|mī}}
# {{head|la|pronoun form|head=mī}}
#


# Clean up use of macrons in Latin lemmas.

import pywikibot, re, sys, codecs, argparse

import blib
from blib import getparam, rmparam, msg, site, tname

la_noun_decl_templates = {
  "la-decl-1st",
  "la-decl-first",
  "la-decl-first-loc",
  "la-decl-1st-1st",
  "la-decl-1st-1st-loc",
  "la-decl-1st-abus",
  "la-decl-1st-am",
  "la-decl-1st-Greek",
  "la-decl-1st-Greek-Ma",
  "la-decl-1st-Greek-Me",
  "la-decl-2nd",
  "la-decl-second",
  "la-decl-2nd-2nd",
  "la-decl-2nd-er",
  "la-decl-2nd-Greek",
  "la-decl-2nd-N-ium",
  "la-decl-2nd-ius",
  "la-decl-2nd-N",
  "la-decl-2nd-N-Greek",
  "la-decl-2nd-N-us",
  "la-decl-3rd",
  "la-decl-3rd-Greek",
  "la-decl-3rd-Greek-er",
  "la-decl-3rd-Greek-on-M",
  "la-decl-3rd-Greek-s",
  "la-decl-3rd-I",
  "la-decl-3rd-I-ignis",
  "la-decl-3rd-I-navis",
  "la-decl-3rd-N",
  "la-decl-3rd-N-I",
  "la-decl-3rd-N-I-pure",
  "la-decl-3rd-polis",
  "la-decl-4th",
  "la-decl-4th-argo",
  "la-decl-4th-echo",
  "la-decl-4th-loc+2nd-adj",
  "la-decl-4th-N",
  "la-decl-4th-N-ubus",
  "la-decl-4th-ubus",
  "la-decl-5th",
  "la-decl-5th-i",
  "la-decl-5th-VOW",
  "la-decl-indecl",
}

cases = [ "nom", "gen", "dat", "acc", "abl", "voc", "loc" ]

la_noun_decl_overrides = [ "%s_%s" % (case, num)
    for num in ["sg", "pl"] for case in cases]

la_adj_decl_overrides = [ "%s_%s_%s" % (case, num, g)
    for g in ["m", "f", "n"]
    for num in ["sg", "pl"]
    for case in cases
]

la_adj_head_templates = {
  "la-adj-1&2",
  "la-adj-3rd-1E",
  "la-adj-3rd-2E",
  "la-adj-3rd-3E",
  "la-adj-comparative",
  "la-adj-superlative",
}

la_adj_decl_templates = {
  "la-decl-1&2",
  "la-decl-3rd-1E",
  "la-decl-3rd-2E",
  "la-decl-3rd-3E",
}

demacron_mapper = {
  u'ā': 'a',
  u'ē': 'e',
  u'ī': 'i',
  u'ō': 'o',
  u'ū': 'u',
  u'ȳ': 'y',
  u'Ā': 'A',
  u'Ē': 'E',
  u'Ī': 'I',
  u'Ō': 'O',
  u'Ū': 'U',
  u'Ȳ': 'Y',
  u'ă': 'a',
  u'ĕ': 'e',
  u'ĭ': 'i',
  u'ŏ': 'o',
  u'ŭ': 'u',
  # no composed breve-y
  u'Ă': 'A',
  u'Ĕ': 'E',
  u'Ĭ': 'I',
  u'Ŏ': 'O',
  u'Ŭ': 'U',
  # combining breve
  u'\u0306': '',
  u'ë', 'e',
  u'Ë', 'E',
}

def remove_macrons(text):
  return re.sub(u'([āēīōūȳĀĒĪŌŪȲăĕĭŏŭĂĔĬŎŬ\u0306ëË])', lambda m: demacron_mapper[m.group(1)], text)

def get_references(page, start, end, include_page=False):
  global args
  if args.dry_run:
    msg("Getting references to %s" % page)
    return []
  else:
    return blib.references(remove_macrons(page), start, end, include_page=include_page)

def frob_stem(t, param, stem, split_slashes=False):
  origt = unicode(t)
  origval = getparam(t, param)
  if split_slashes:
    vals = origval.split("/")
  else:
    vals = [origval]
  newvals = []
  for val in vals:
    no_macrons_val = remove_macrons(val)
    assert len(no_macrons_val) == len(val)
    if type(stem) is not list:
      stem = [stem]
    for st in stem:
      no_macrons_stem = remove_macrons(st)
      assert len(no_macrons_stem) == len(st)
      if no_macrons_val.startswith(no_macrons_stem):
        newval = st + val[len(st):]
        newvals.append(newval)
        break
      else:
        newvals.append(val)
  newval = "/".join(newvals)
  if newval != origval:
    t.add(param, newval)
    pagemsg("Replaced %s with %s" % (origt, unicode(t)))
    notes.append("updated macrons in %s" % tname(t))

def frob_exact(t, param, newval):
  origt = unicode(t)
  val = getparam(t, param)
  no_macrons_val = remove_macrons(val)
  assert len(no_macrons_val) == len(val)
  no_macrons_newval = remove_macrons(newval)
  assert len(no_macrons_newval) == len(newval)
  if no_macrons_val == no_macrons_newval:
    if newval != val:
      t.add(param, newval)
      pagemsg("Replaced %s with %s" % (origt, unicode(t)))
      notes.append("updated macrons in %s" % tname(t))

def frob_chain_stem(t, param, stem, split_slashes=False):
  if type(param) is list:
    first, rest = param
  else:
    first = param
    rest = param
  if first:
    frob_stem(t, first, stem, split_slashes=split_slashes)
  num = 2
  while getparam(t, "%s%s" % (rest, num)):
    frob_stem(t, "%s%s" % (rest, num), stem, split_slashes=split_slashes)

def frob_chain_exact(t, param, newval):
  if type(param) is list:
    first, rest = param
  else:
    first = param
    rest = param
  if first:
    frob_exact(t, first, newval)
  num = 2
  while getparam(t, "%s%s" % (rest, num)):
    frob_exact(t, "%s%s" % (rest, num), newval)

def process_page(index, page, pos, lemma, stem, save, verbose):
  pagetitle = unicode(page.title())
  def pagemsg(txt):
    msg("Page %s %s: %s" % (index, pagetitle, txt))

  def expand_text(tempcall):
    return blib.expand_text(tempcall, pagetitle, pagemsg, verbose)

  pagemsg("Processing")

  text = unicode(page.text)
  parsed = blib.parse(page)
  notes = []
  for t in parsed.filter_templates():
    origt = unicode(t)
    tn = tname(t)

    if tn == "la-IPA":
      if not getparam(t, "1"):
        if remove_macrons(lemma) != lemma:
          eccl = getparam(t, "eccl")
          rmparam(t, "eccl")
          t.add("1", lemma)
          if eccl:
            t.add("eccl", eccl)
          notes.append("add pronunciation to {{la-IPA}}")
      else:
        frob_stem(t, "1", stem)
    elif tn == "la-noun-form" and pos == "noun":
      frob_stem(t, "1", stem)
    elif tn == "la-adj-form" and pos == "adj":
      frob_stem(t, "1", stem)
    elif tn == "la-num-form" and pos == "numadj":
      frob_stem(t, "1", stem)
    elif tn in ["la-perfect participle", "la-future participle",
        "la-gerundive",
        "la-verb-form", "la-part-form", "la-decl-1&2",
        "la-decl-3rd-part"] and pos == "verb":
      frob_stem(t, "1", stem)
    elif tn == "la-present participle" and pos == "verb":
      # la-present participle has a different format
      stems = stem
      if type(stems) is not list:
        stems = [stem]
      stems = [re.sub(u"[āē]ns$", "", st) for st in stems]
      for st in stems:
        frob_exact(t, "1", st)
    elif tn == "inflection of":
      lang = getparam(t, "lang")
      if lang:
        lemma_param = "1"
        alt_param = "2"
      else:
        lang = getparam(t, "1")
        lemma_param = "2"
        alt_param = "3"
      if lang == "la":
        tlemma = getparam(t, lemma_param)
        talt = getparam(t, alt_param)
        if not talt:
          frob_exact(t, lemma_param, lemma)
        elif remove_macrons(tlemma) == remove_macrons(talt):
          t.add(lemma_param, talt)
          t.add(alt_param, "")
          notes.append("moved alt param in {{inflection of|la}} to lemma")
          frob_exact(t, lemma_param, lemma)
        else:
          pagemsg("WARNING: In %s, alt param != lemma param even aside from macrons" % origt)
          frob_exact(t, alt_param, lemma)

  return unicode(parsed), notes

def process_lemma(index, page, pos, lemma, stem, save, verbose):
  pagetitle = unicode(page.title())
  def pagemsg(txt):
    msg("Page %s %s: %s" % (index, pagetitle, txt))

  def expand_text(tempcall):
    return blib.expand_text(tempcall, pagetitle, pagemsg, verbose)

  pagemsg("Processing")

  text = unicode(page.text)
  parsed = blib.parse(page)
  notes = []
  for t in parsed.filter_templates():
    origt = unicode(t)
    tn = tname(t)

    if tn == "la-IPA":
      if not getparam(t, "1"):
        if remove_macrons(lemma) != lemma:
          eccl = getparam(t, "eccl")
          rmparam(t, "eccl")
          t.add("1", lemma)
          if eccl:
            t.add("eccl", eccl)
          notes.append("add pronunciation to {{la-IPA}}")
      else:
        frob_stem(t, "1", stem)
    elif tn == "la-noun" and pos == "noun":
      frob_exact(t, "1", lemma)
      frob_stem(t, "2", stem)
      frob_chain_stem(t, [None, "gen"], stem)
    elif tn in la_noun_decl_templates and pos == "noun":
      frob_stem(t, "1", stem)
      frob_stem(t, "2", stem)
      for override in la_noun_decl_overrides:
        frob_stem(t, override, stem, split_slashes=True)
    elif tn in ["la-adj-1&2", "la-adj-3rd-3E"] and pos == "adj":
      frob_stem_chain(t, ["1", "head"], stem)
      frob_stem_chain(t, ["2", "f"], stem)
      frob_stem_chain(t, ["3", "n"], stem)
      frob_stem_chain(t, "comp", stem)
      frob_stem_chain(t, "sup", stem)
    elif tn in "la-adj-3rd-1E" and pos == "adj":
      frob_stem_chain(t, ["1", "head"], stem)
      frob_stem_chain(t, ["2", "gen"], stem)
      frob_stem_chain(t, "comp", stem)
      frob_stem_chain(t, "sup", stem)
    elif tn in "la-adj-3rd-2E" and pos == "adj":
      frob_stem_chain(t, ["1", "head"], stem)
      frob_stem_chain(t, ["2", "n"], stem)
      frob_stem_chain(t, "comp", stem)
      frob_stem_chain(t, "sup", stem)
    elif tn in la_adj_decl_templates and pos == "adj":
      frob_stem(t, "1", stem)
      frob_stem(t, "2", stem)
      for override in la_noun_decl_overrides:
        frob_stem(t, override, stem, split_slashes=True)
    elif tn == "la-adj-form" and pos == "adj":
      frob_stem(t, "1", stem)
    elif tn == "la-num-form" and pos == "numadj":
      frob_stem(t, "1", stem)
    elif tn in ["la-perfect participle", "la-future participle",
        "la-gerundive",
        "la-verb-form", "la-part-form", "la-decl-1&2",
        "la-decl-3rd-part"] and pos == "verb":
      frob_stem(t, "1", stem)
    elif tn == "la-present participle" and pos == "verb":
      # la-present participle has a different format
      stems = stem
      if type(stems) is not list:
        stems = [stem]
      stems = [re.sub(u"[āē]ns$", "", st) for st in stems]
      for st in stems:
        frob_exact(t, "1", st)
    elif tn == "inflection of":
      lang = getparam(t, "lang")
      if lang:
        lemma_param = "1"
        alt_param = "2"
      else:
        lang = getparam(t, "1")
        lemma_param = "2"
        alt_param = "3"
      if lang == "la":
        tlemma = getparam(t, lemma_param)
        talt = getparam(t, alt_param)
        if not talt:
          frob_exact(t, lemma_param, lemma)
        elif remove_macrons(tlemma) == remove_macrons(talt):
          t.add(lemma_param, talt)
          t.add(alt_param, "")
          notes.append("moved alt param in {{inflection of|la}} to lemma")
          frob_exact(t, lemma_param, lemma)
        else:
          pagemsg("WARNING: In %s, alt param != lemma param even aside from macrons" % origt)
          frob_exact(t, alt_param, lemma)

  return unicode(parsed), notes


parser = blib.create_argparser("Clean up usage of macrons in Latin non-lemma forms")
parser.add_argument("--direcfile", help="File containing directives of lemmas to process.")
parser.add_argument("--dry-run", help="Just show what would be checked, don't actually check references.", action="store_true")
args = parser.parse_args()
start, end = blib.parse_start_end(args.start, args.end)

direcfile = args.direcfile.decode("utf-8")
for line in codecs.open(direcfile, "r", "utf-8"):
  line = line.rstrip('\n')
  if line.startswith("*"):
    line = line[1:]
    msg("Need to investigate: %s" % line)
  if line.startswith("#"):
    continue
  parts = line.split(" ")
  if len(parts) == 2 and parts[0] in ["adv", "num", "phrase"]:
    pass
  elif len(parts) == 2 or len(parts) == 3 and parts[0] in ["n3", "a3"]:
    # noun or adjective
    if len(parts) == 3:
      pos, lemma, stem = parts
    else:
      pos, lemma = parts
      stem = None
    if pos == "n1":
      assert lemma.endswith("a")
      pos = "noun"
      stem = lemma[:-1]
    elif pos == "n2":
      assert lemma.endswith("us") or lemma.endswith("um")
      pos = "noun"
      if lemma.endswith("ius"):
        stem = [lemma[:-2], lemma[:-3] + u"ī"]
      else:
        stem = lemma[:-2]
    elif pos == "n3":
      pos = "noun"
      if stem:
        pass
      elif lemma.endswith(u"iō"):
        stem = lemma + "n"
      elif lemma.endswith("or"):
        stem = lemma[:-2] + u"ōr"
      elif lemma.endswith(u"īx"):
        stem = lemma[:-1] + "c"
      elif lemma.endswith(u"tās"):
        stem = lemma[:-1] + "t"
      elif lemma.endswith(u"ūdō"):
        stem = lemma[:-1] + "in"
      else:
        raise ValueError("Don't recognize n3 lemma %s" % lemma)
    elif pos == "n4":
      assert lemma.endswith("us")
      pos = "noun"
      stem = lemma[:-2]
    elif pos == "a1":
      assert lemma.endswith("us")
      pos = "adj"
      if lemma.endswith("ius"):
        stem = [lemma[:-2], lemma[:-3] + u"ī"]
      else:
        stem = lemma[:-2]
    elif pos == "a3":
      pos = "adj"
      if stem:
        pass
      elif lemma.endswith("is"):
        stem = lemma[:-2]
      elif lemma.endswith(u"āx") or lemma.endswith(u"īx"):
        stem = lemma[:-1] + "c"
      elif lemma.endswith(u"āns"):
        stem = lemma[:-3] + "ant"
      elif lemma.endswith(u"ēns"):
        stem = lemma[:-3] + "ent"
      elif lemma.endswith("ior"):
        stem = [lemma[:-2] + "us", lemma[:-2] + u"ōr"]
      else:
        raise ValueError("Don't recognize a3 lemma %s" % lemma)
    elif pos == "num1":
      assert lemma.endswith(u"ī")
      pos = "numadj"
      stem = lemma[:-1]
    else:
      raise ValueError("Don't recognize pos spec %s" % pos)
    def do_process_page(page, index, parsed):
      return process_page(index, page, pos, lemma, stem, args.save, args.verbose)
    for index, page in get_references(lemma, start, end):
      stems = stem
      if type(stems) is not list:
        stems = [stems]
      for st in stems:
        no_macrons_stem = remove_macrons(st)
        assert len(no_macrons_stem) == len(st)
        if unicode(page.title()).startswith(no_macrons_stem):
          blib.do_edit(page, index, do_process_page, save=args.save, verbose=args.verbose)
          break
      else:
        # no break
        msg("Skipped %s for lemma %s because doesn't match stem(s) %s" % (
          unicode(page.title()), lemma, ", ".join(stems)))
  else:
    pos = "verb"
    stems_lemmas = []
    if len(parts) == 3:
      # deponent verb
      lemma, inf, supine = parts
      if lemma.startswith("*"):
        no_main = True
        lemma = lemma[1:]
      else:
        no_main = False
      # do the past passive and future active participles
      if not supine.startswith("*") and not supine.startswith("--"):
        assert supine.endswith("um")
        stems_lemmas.append((supine[:-2], supine[:-2] + "us", True))
        stems_lemmas.append((supine[:-2] + u"ūr", supine[:-2] + u"ūrus", True))
      if not no_main:
        # do the remaining two participles
        assert lemma.endswith(u"or") or lemma.endswith("itur")
        if inf.endswith(u"ārī"):
          short_part_stem = inf[:-3] + "a"
          long_part_stem = inf[:-2]
        elif inf.endswith(u"īrī"):
          short_part_stem = inf[:-3] + "ie"
          long_part_stem = inf[:-3] + u"iē"
        elif inf.endswith(u"ērī"):
          short_part_stem = inf[:-3] + "e"
          long_part_stem = inf[:-2]
        elif inf.endswith(u"ī") and lemma.endswith(u"iō"):
          short_part_stem = inf[:-1] + "ie"
          long_part_stem = inf[:-1] + u"iē"
        elif inf.endswith(u"ī") and lemma.endswith(u"it"):
          # impersonal -ī; don't know whether i-stem or not
          short_part_stem = [inf[:-1] + "e", inf[:-1] + "ie"]
          long_part_stem = [inf[:-1] + u"ē", inf[:-1] + u"iē"]
        else:
          assert inf.endswith(u"ī")
          short_part_stem = inf[:-1] + "e"
          long_part_stem = inf[:-1] + u"ē"
        if type(short_part_stem) is not list:
          short_part_stem = [short_part_stem]
        if type(long_part_stem) is not list:
          long_part_stem = [long_part_stem]
        for shstem, lostem in zip(short_part_stem, long_part_stem):
          stems_lemmas.append((shstem + "nt", lostem + "ns", True))
          stems_lemmas.append((shstem + "nd", shstem + "ndus", True))

        # do the present stem
        if lemma.endswith(u"or"):
          m = re.search(u"^(.*?)([ei]?)or$", lemma)
          assert m
          if inf.endswith(u"ārī"):
            stem = m.group(1) + m.group(2)
          else:
            stem = m.group(1)
        else:
          m = re.search(u"^(.*?)([āēīi])tur$", lemma)
          assert m
          stem = m.group(1)
        stems_lemmas.append((stem, lemma, False))

    else:
      assert len(parts) == 4
      lemma, inf, perf, supine = parts
      if lemma.startswith("*"):
        no_main = True
        lemma = lemma[1:]
      else:
        no_main = False
      # do the past passive and future active participles
      if not supine.startswith("*") and not supine.startswith("--"):
        assert supine.endswith("um")
        stems_lemmas.append((supine[:-2], supine[:-2] + "us", True))
        stems_lemmas.append((supine[:-2] + u"ūr", supine[:-2] + u"ūrus", True))
      # do the perfect stem
      if not perf.startswith("*") and not perf.startswith("--"):
        assert perf.endswith(u"ī") or perf.endswith("it")
        if perf.endswith(u"ī"):
          stems_lemmas.append((perf[:-1], lemma, False))
        else:
          # impersonal
          stems_lemmas.append((perf[:-2], lemma, False))
      if not no_main:
        # do the remaining two participles
        assert lemma.endswith(u"ō") or lemma.endswith("t")
        if inf.endswith(u"āre"):
          short_part_stem = inf[:-3] + "a"
          long_part_stem = inf[:-2]
        elif inf.endswith(u"īre") or inf.endswith("ere") and lemma.endswith(u"iō"):
          short_part_stem = inf[:-3] + "ie"
          long_part_stem = inf[:-3] + u"iē"
        elif inf.endswith(u"ere") and lemma.endswith(u"it"):
          # impersonal -ere; don't know whether i-stem or not
          short_part_stem = [inf[:-3] + "e", inf[:-3] + "ie"]
          long_part_stem = [inf[:-3] + u"ē", inf[:-3] + u"iē"]
        else:
          assert inf.endswith("ere") or inf.endswith(u"ēre")
          short_part_stem = inf[:-3] + "e"
          long_part_stem = inf[:-3] + u"ē"
        if type(short_part_stem) is not list:
          short_part_stem = [short_part_stem]
        if type(long_part_stem) is not list:
          long_part_stem = [long_part_stem]
        for shstem, lostem in zip(short_part_stem, long_part_stem):
          stems_lemmas.append((shstem + "nt", lostem + "ns", True))
          stems_lemmas.append((shstem + "nd", shstem + "ndus", True))

        # do the present stem
        if lemma.endswith(u"ō"):
          m = re.search(u"^(.*?)([ei]?)ō$", lemma)
          assert m
          if inf.endswith(u"āre"):
            stem = m.group(1) + m.group(2)
          else:
            stem = m.group(1)
        else:
          m = re.search(u"^(.*?)([aei])t$", lemma)
          assert m
          stem = m.group(1)
        stems_lemmas.append((stem, lemma, False))

    for stem, lemma, include_page in stems_lemmas:
      def do_process_page(page, index, parsed):
        return process_page(index, page, pos, lemma, stem, args.save, args.verbose)
      for index, page in get_references(lemma, start, end,
          include_page=include_page):
        no_macrons_stem = remove_macrons(stem)
        assert len(no_macrons_stem) == len(stem)
        if unicode(page.title()).startswith(no_macrons_stem):
          blib.do_edit(page, index, do_process_page, save=args.save, verbose=args.verbose)
        else:
          msg("Skipped %s for lemma %s because doesn't match stem %s" % (
            unicode(page.title()), lemma, stem))
